{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fddcffea",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfb0f08",
   "metadata": {},
   "source": [
    "<p><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/5/50/Oracle_logo.svg/2560px-Oracle_logo.svg.png\" width=\"200\" align = \"left\"></p>\n",
    "\n",
    "## **<h1 align =\"middle\"><b>Lab 5 (Optional)</b></h1>**\n",
    "### **<h1 align =\"middle\"><b>Store and Deploy the end-to-end script as HTTP endpoint</b></h1>**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f4f372",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22263fe6",
   "metadata": {},
   "source": [
    "## **Overview steps in Lab 5**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797d4ee6",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><font size=\"4\">Step 1 - Add your OCI Document Understanding variables </font></summary>\n",
    "\n",
    "```md\n",
    "   In the same way as you did in Lab 4, you have to change/add your variables like bucket name, namespace, prefix, and the custom Model OCIDS to the final script. The script is named 'score.py' and will be used for inferences.\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><font size=\"4\">Step 2 - Store the model in the model catalog</font></summary>\n",
    "\n",
    "```md\n",
    "   In this step, all the files in the model_artifacts folder will be stored as a model in the model catalog.\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><font size=\"4\">Step 3 - Deploy the model in the Oracle Cloud Console</font></summary>\n",
    "\n",
    "```md\n",
    "   In this step, you will deploy the stored model using the Oracle Cloud console\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><font size=\"4\">Step 4 - Invoke the Deployed model, HTTP endpoint</font></summary>\n",
    "\n",
    "```md\n",
    "   In this final step, you will swap the newly created HTTP endpoint and invoke the full endpoint with an encoded PDF file and visualize the results in a table.\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be68248d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00de6ae",
   "metadata": {},
   "source": [
    "## **Step 1 - Add your OCI Document Understanding variables**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d42c8e0",
   "metadata": {},
   "source": [
    "#### On the left, in the directory, you should see a file named 'score.py'. Open the file and change the following variables (in the same way as in the previous lab). Add your variables between the empty brackets.\n",
    "- model_1_classification = \"\"\n",
    "- model_2_wholefoods_kv = \"\"\n",
    "- model_3_walgreens_kv = \"\"\n",
    "- bucket_name = \"\"\n",
    "- namespace = \"\"\n",
    "- compartment_ocid = \"\"\n",
    "- output_name_prefix = \"\"\n",
    "\n",
    "#### When done, **save the file** by either by closing the file and click \"Save\" or perform CTRL+S."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cec2ced",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a13b59a",
   "metadata": {},
   "source": [
    "## **Step 2 - Store the model in the model catalog**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f7c636",
   "metadata": {},
   "source": [
    "### **2.1 Create templates for model catalog**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da63d24b",
   "metadata": {},
   "source": [
    "#### Run the below cell. This will create a new directory named **'model_artifacts'** with several boilerplates. Make sure to use the same 'generalml_p38_cpu_v1' Conda environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4afcc82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -R model_artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "261edf96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "algorithm: null\n",
       "artifact_dir:\n",
       "  /home/datascience/model_artifacts:\n",
       "  - - score.py\n",
       "    - runtime.yaml\n",
       "    - model.pkl\n",
       "framework: null\n",
       "model_deployment_id: null\n",
       "model_id: null"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ads.model.generic_model import GenericModel\n",
    "\n",
    "class Toy:\n",
    "    def predict(self, x):\n",
    "        return x ** 2\n",
    "model = Toy()\n",
    "\n",
    "generic_model = GenericModel(estimator=model, artifact_dir=\"model_artifacts\")\n",
    "generic_model.summary_status()\n",
    "\n",
    "generic_model.prepare(\n",
    "    inference_conda_env='oci://service-conda-packs@id19sfcrra6z/service_pack/gpu/PyTorch_2.0_for_GPU_on_Python_3.9/2.0/pytorch20_p39_gpu_v2',\n",
    "    inference_python_version='3.8',\n",
    "        force_overwrite=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e470ee",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfc3ea1",
   "metadata": {},
   "source": [
    "### **2.2 Add your config, private key, and the score.py to the 'model_artifacts' directory**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55e32dd",
   "metadata": {},
   "source": [
    "#### Run the below cell, this will\n",
    "- 1. Copy the config to the model_artifacts directory\n",
    "- 2. Copy the private key to the model_artifacts directory\n",
    "- 3. Delete the boilerplate score.py\n",
    "- 4. Copy the score.py from the main directory to the model_artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "58082eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy the config file and private key to the model_artifacts directory\n",
    "!cp -R ./config ./model_artifacts\n",
    "!cp -R ./private_key.pem ./model_artifacts\n",
    "\n",
    "#remove the current score.py\n",
    "!rm -R ./model_artifacts/score.py\n",
    "\n",
    "#copy the new score.py\n",
    "!cp -R ./score.py ./model_artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba77469",
   "metadata": {},
   "source": [
    "### **2.3 Store the model in the Model Catalog**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee5944c",
   "metadata": {},
   "source": [
    "#### Run the below cell. This will review the artifacts file. All lines should have \"Passed\" int he \"Result\" column. If so, proceed to next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b4233c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', 'score.py', 'config', 'runtime.yaml', 'private_key.pem', 'model.pkl']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test key</th>\n",
       "      <th>Test name</th>\n",
       "      <th>Result</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>runtime_env_path</td>\n",
       "      <td>Check that field MODEL_DEPLOYMENT.INFERENCE_ENV_PATH is set</td>\n",
       "      <td>Passed</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>runtime_env_python</td>\n",
       "      <td>Check that field MODEL_DEPLOYMENT.INFERENCE_PYTHON_VERSION is set to a value of 3.6 or higher</td>\n",
       "      <td>Passed</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>runtime_path_exist</td>\n",
       "      <td>Check that the file path in MODEL_DEPLOYMENT.INFERENCE_ENV_PATH is correct.</td>\n",
       "      <td>Passed</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>runtime_version</td>\n",
       "      <td>Check that field MODEL_ARTIFACT_VERSION is set to 3.0</td>\n",
       "      <td>Passed</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>runtime_yaml</td>\n",
       "      <td>Check that the file \"runtime.yaml\" exists and is in the top level directory of the artifact directory</td>\n",
       "      <td>Passed</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>score_load_model</td>\n",
       "      <td>Check that load_model() is defined</td>\n",
       "      <td>Passed</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>score_predict</td>\n",
       "      <td>Check that predict() is defined</td>\n",
       "      <td>Passed</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>score_predict_arg</td>\n",
       "      <td>Check that all other arguments in predict() are optional and have default values</td>\n",
       "      <td>Passed</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>score_predict_data</td>\n",
       "      <td>Check that the only required argument for predict() is named \"data\"</td>\n",
       "      <td>Passed</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>score_py</td>\n",
       "      <td>Check that the file \"score.py\" exists and is in the top level directory of the artifact directory</td>\n",
       "      <td>Passed</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>score_syntax</td>\n",
       "      <td>Check for Python syntax errors</td>\n",
       "      <td>Passed</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Test key  \\\n",
       "0     runtime_env_path   \n",
       "1   runtime_env_python   \n",
       "2   runtime_path_exist   \n",
       "3      runtime_version   \n",
       "4         runtime_yaml   \n",
       "5     score_load_model   \n",
       "6        score_predict   \n",
       "7    score_predict_arg   \n",
       "8   score_predict_data   \n",
       "9             score_py   \n",
       "10        score_syntax   \n",
       "\n",
       "                                                                                                Test name  \\\n",
       "0                                             Check that field MODEL_DEPLOYMENT.INFERENCE_ENV_PATH is set   \n",
       "1           Check that field MODEL_DEPLOYMENT.INFERENCE_PYTHON_VERSION is set to a value of 3.6 or higher   \n",
       "2                             Check that the file path in MODEL_DEPLOYMENT.INFERENCE_ENV_PATH is correct.   \n",
       "3                                                   Check that field MODEL_ARTIFACT_VERSION is set to 3.0   \n",
       "4   Check that the file \"runtime.yaml\" exists and is in the top level directory of the artifact directory   \n",
       "5                                                                      Check that load_model() is defined   \n",
       "6                                                                         Check that predict() is defined   \n",
       "7                        Check that all other arguments in predict() are optional and have default values   \n",
       "8                                     Check that the only required argument for predict() is named \"data\"   \n",
       "9       Check that the file \"score.py\" exists and is in the top level directory of the artifact directory   \n",
       "10                                                                         Check for Python syntax errors   \n",
       "\n",
       "    Result Message  \n",
       "0   Passed          \n",
       "1   Passed          \n",
       "2   Passed          \n",
       "3   Passed          \n",
       "4   Passed          \n",
       "5   Passed          \n",
       "6   Passed          \n",
       "7   Passed          \n",
       "8   Passed          \n",
       "9   Passed          \n",
       "10  Passed          "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generic_model.introspect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e0ecb8",
   "metadata": {},
   "source": [
    "#### Run the below cell. This will store the model in the model catalog. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "477542d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['score.py', 'config', 'runtime.yaml', 'test_json_output.json', 'private_key.pem', 'model.pkl']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop1:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Saving the model artifact to the model catalog. \n",
    "catalog_entry = generic_model.save(display_name='full_model_v5', description='full_model_v5', timeout=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67513e57",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa843a02",
   "metadata": {},
   "source": [
    "## **Step 3 - Deploy the model in the Oracle Cloud Console**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92795f75",
   "metadata": {},
   "source": [
    "#### Please go back to the **Oracle Cloud console**. See the LiveLab for the steps with screenshots.\n",
    "- 1. In Oracle Cloud, click on the **Hamburger menu**, go to **Analytics and AI**\n",
    "- 2. Click on \"Data Science\". Step into your **Project** in the correct comparment.\n",
    "- 3. On the left, click on **\"Models\"**. Click on the name **full_model_v1\" model**.\n",
    "- 4. Click on **\"More actions\"** and following on **\"Create model deployment\"**\n",
    "- 5. Add a name to **\"Name\"**. Click on **\"Create\"**. This will create a Model Deployment (HTTP endpoint) for your stored model in the model catalog\n",
    "- 6. Optionally, you may add logging to debug when the model deployment fails."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed68ee9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399d0981",
   "metadata": {},
   "source": [
    "## **Step 4 - Invoke the Deployed model, HTTP endpoint**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca100fae",
   "metadata": {},
   "source": [
    "### **4.1 Get the HTTP endpoint**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845bde30",
   "metadata": {},
   "source": [
    "#### \n",
    "- 1. When the Model Deployment is **Active**, step into the Model Deployment and on the left, click on **\"Invoking your model\"**\n",
    "- 2. Copy the **HTTP endpoint** and swap the HTTP endpoint in the below cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9b1903fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add your HTTP endpoint here and run the cell\n",
    "http_endpoint = \"https://modeldeployment.eu-frankfurt-1.oci.customer-oci.com/ocid1.datasciencemodeldeployment.oc1.eu-frankfurt-1.amaaaaaangencdyajhit3zhiqp4z2kbus3ivroh6y366b5qwq5tysvcrlszq/predict\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14354e14",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08529636",
   "metadata": {},
   "source": [
    "### **4.2 Invoke the HTTP endpoint**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e46ef1",
   "metadata": {},
   "source": [
    "#### Run the below cell. This will use your HTTP endpoint, encode a PDF file, and invoke your HTTP endpoint. The results are shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2b04e391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [500]>\n",
      "ERROR:ads.common:ADS Exception\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/datascience/conda/generalml_p38_cpu_v1/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3457, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_17290/2486689441.py\", line 45, in <module>\n",
      "    df = pd.read_json(results_df_json)\n",
      "  File \"/home/datascience/conda/generalml_p38_cpu_v1/lib/python3.8/site-packages/pandas/util/_decorators.py\", line 207, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/datascience/conda/generalml_p38_cpu_v1/lib/python3.8/site-packages/pandas/util/_decorators.py\", line 311, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/datascience/conda/generalml_p38_cpu_v1/lib/python3.8/site-packages/pandas/io/json/_json.py\", line 612, in read_json\n",
      "    return json_reader.read()\n",
      "  File \"/home/datascience/conda/generalml_p38_cpu_v1/lib/python3.8/site-packages/pandas/io/json/_json.py\", line 746, in read\n",
      "    obj = self._get_object_parser(self.data)\n",
      "  File \"/home/datascience/conda/generalml_p38_cpu_v1/lib/python3.8/site-packages/pandas/io/json/_json.py\", line 768, in _get_object_parser\n",
      "    obj = FrameParser(json, **kwargs).parse()\n",
      "  File \"/home/datascience/conda/generalml_p38_cpu_v1/lib/python3.8/site-packages/pandas/io/json/_json.py\", line 880, in parse\n",
      "    self._parse_no_numpy()\n",
      "  File \"/home/datascience/conda/generalml_p38_cpu_v1/lib/python3.8/site-packages/pandas/io/json/_json.py\", line 1132, in _parse_no_numpy\n",
      "    self.obj = DataFrame(\n",
      "  File \"/home/datascience/conda/generalml_p38_cpu_v1/lib/python3.8/site-packages/pandas/core/frame.py\", line 636, in __init__\n",
      "    mgr = dict_to_mgr(data, index, columns, dtype=dtype, copy=copy, typ=manager)\n",
      "  File \"/home/datascience/conda/generalml_p38_cpu_v1/lib/python3.8/site-packages/pandas/core/internals/construction.py\", line 502, in dict_to_mgr\n",
      "    return arrays_to_mgr(arrays, columns, index, dtype=dtype, typ=typ, consolidate=copy)\n",
      "  File \"/home/datascience/conda/generalml_p38_cpu_v1/lib/python3.8/site-packages/pandas/core/internals/construction.py\", line 120, in arrays_to_mgr\n",
      "    index = _extract_index(arrays)\n",
      "  File \"/home/datascience/conda/generalml_p38_cpu_v1/lib/python3.8/site-packages/pandas/core/internals/construction.py\", line 664, in _extract_index\n",
      "    raise ValueError(\"If using all scalar values, you must pass an index\")\n",
      "ValueError: If using all scalar values, you must pass an index\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ValueError: If using all scalar values, you must pass an index"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import oci\n",
    "from oci.signer import Signer\n",
    "import json\n",
    "import base64\n",
    "import pandas as pd\n",
    "\n",
    "## Add your HTTP endpoint here and run the cell\n",
    "http_endpoint = \"https://modeldeployment.eu-frankfurt-1.oci.customer-oci.com/ocid1.datasciencemodeldeployment.oc1.eu-frankfurt-1.amaaaaaangencdyajhit3zhiqp4z2kbus3ivroh6y366b5qwq5tysvcrlszq/predict\"\n",
    "\n",
    "#################################################################################### 1. Create a JSON payload to send to the HTTP endpoint. This is an encoded PDF file. \n",
    "#################################################################################### \n",
    "#################################################################################### \n",
    "\n",
    "document = \"./walgreens_1.pdf\"\n",
    "document_no_ext = document[:-4]\n",
    "\n",
    "with open(document, \"rb\") as document_file:\n",
    "    document_encoded = base64.b64encode(document_file.read()).decode('utf-8')\n",
    "    \n",
    "    \n",
    "#create full json input for model\n",
    "full_json_input = {'data':document_encoded}\n",
    "\n",
    "\n",
    "#################################################################################### 2. Use the Config file for authentication\n",
    "#################################################################################### \n",
    "#################################################################################### \n",
    "\n",
    "config = oci.config.from_file(\"./config\") \n",
    "auth = Signer(\n",
    "        tenancy=config['tenancy'],\n",
    "        user=config['user'],\n",
    "        fingerprint=config['fingerprint'],\n",
    "        private_key_file_location=config['key_file'],\n",
    "        pass_phrase=config['pass_phrase'])\n",
    "\n",
    "\n",
    "#################################################################################### 3. Make the POST request and visualize results in table\n",
    "#################################################################################### \n",
    "#################################################################################### \n",
    "\n",
    "response = requests.post(http_endpoint, json=full_json_input, auth=auth)\n",
    "print(response)\n",
    "\n",
    "#load response in dataframe\n",
    "results_df_json = (json.loads(response.content))\n",
    "df = pd.read_json(results_df_json)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1d14c4fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"code\": \"InternalServerError\", \"message\": \"OpenSSL 3.0\\'s legacy provider failed to load. This is a fatal error by default, but cryptography supports running without legacy algorithms by setting the environment variable CRYPTOGRAPHY_OPENSSL_NO_LEGACY. If you did not expect this error, you have likely made a mistake with your OpenSSL configuration.\", \"status\": \"OpenSSL 3.0\\'s legacy provider failed to load. This is a fatal error by default, but cryptography supports running without legacy algorithms by setting the environment variable CRYPTOGRAPHY_OPENSSL_NO_LEGACY. If you did not expect this error, you have likely made a mistake with your OpenSSL configuration.\"}'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5aa783e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n",
      "2.119.1\n"
     ]
    }
   ],
   "source": [
    "import ads\n",
    "import oci\n",
    "print(ads.__version__)\n",
    "print(oci.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e00b128",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR - Exception\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/datascience/conda/generalml_p38_cpu_v1/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3457, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_12567/3143847521.py\", line 1, in <module>\n",
      "    from oci.generative_ai.models import generate_text_details\n",
      "ImportError: cannot import name 'generate_text_details' from 'oci.generative_ai.models' (/home/datascience/conda/generalml_p38_cpu_v1/lib/python3.8/site-packages/oci/generative_ai/models/__init__.py)\n",
      "ImportError: cannot import name 'generate_text_details' from 'oci.generative_ai.models' (/home/datascience/conda/generalml_p38_cpu_v1/lib/python3.8/site-packages/oci/generative_ai/models/__init__.py)"
     ]
    }
   ],
   "source": [
    "from oci.generative_ai.models import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba529519",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83792146",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:generalml_p38_cpu_v1]",
   "language": "python",
   "name": "conda-env-generalml_p38_cpu_v1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
